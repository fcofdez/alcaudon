\chapter{State of the art}
\label{chap:stateoftheart}

\drop{T}{his} chapter aims to explain the concepts and techniques in which
Alcaudon is based on. As shown in Figure~\ref{fig:mindmap}. the project has foundations in
distributed systems, job scheduling, library design and data processing. In the
next sections, those concepts will be analyzed in more depth.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=1\textwidth]{mindmap.pdf}
\caption{Alcaudon foundations}
\label{fig:mindmap}
\end{center}
\end{figure}

\section{Data Processing}

According to a recent study by \FIXME{cita y referencia juntas no
  quedan bien} Cisco~\cite{ciscosurvey}~\ref{fig:ciscodata}
data storage is growing by 40\% yearly. This implies that that by 2020
datacenters will store over 1000 ExaBytes of information. Keeping that data in
silos without performing any use of it is a poor investment.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{ciscodata.png}
\caption{Actual Data Stored in Data Centers~\cite{ciscosurvey}}
\label{fig:ciscodata}
\end{center}
\end{figure}

Processing those amounts of data using traditionals RDBMS is not practical, as
they are not designed to work with that large load of data. In the last years
there have been many ongoing efforts on providing tools to process and analyze
large volumes of data. Some examples could be MapReduce~\cite{mapreduce},
Spark~\cite{spark} and many others. Before those frameworks appeared, there were
some systems capable of processing large amounts of data, like the
supercomputers or HPC. One of the main drawbacks of HPC is accessibility to
those high end computers. Acquiring a new \FIXME{por qué en
  mayúscula?} Supercomputer in the event of a peak
on data production is not feasible. For companies, making an investment in a
super computer is a big risk that in some cases will not be reverted as profit.
Another interesting point to make is that Supercomputers perform well many
floating-point operations per second. However, in general, the computations
executed in systems like MapReduce are simple strings manipulation, counting and
so on. Given these usage patterns, it seems that supercomputers are more suited
to perform scientific analysis.

Once the limitations of HPC for \textit{big data} scenarios have been described,
it is possible to explore how modern internet companies are using computing
resources to process data. For new organizations, it is more sensible to take
advantage of commodity hardware to process large amounts of data. One of the
reasons is that there are many tools to distribute data processing among large
clusters of commodity machines. Once that job is done, those machines can be
turned off or used for other purposes. Working with those systems provides
flexibility, both in resource usage and innovation capacity.

Even though systems such as Map-Reduce have been quite useful to
process large amounts of data, they start to have limitations in
certain aspects. Currently, the need of low latency in data processing
is getting mandatory. Organizations need to get insights about their
data as soon as possible, in order to react quickly to new trends. The
systems described previously are classified as batch systems, as they
are oriented to process historical datasets and not \textit{real time
  data}. There have been some adaptations to these systems, using
approaches like micro-batching where data is processed in smaller
batches, however the latency is still big. These new needs for data
processing have lead to the design and development of systems
specialized in unbounded data processing. These systems try to reduce
the time between the event generation and its processing. \FIXME
{deberías evitar referencias a Alcaudon en este capítulo. Estás
  contando lo que hay para usarlo como argumento para la necesidad del
  nuevo proyecto. Un estado del arte debe tener valor en si mismo
  aunque no escribieras nada más} Alcaudon fits in this category of
data processing systems, as one of its aims is being able to process
unbounded data sets with low latency.

\subsection{Cloud Computing}

Cloud computing might be seen as a recent development. However, during the 1990s
researchers at Xerox PARC started to develop ideas around what they named
\textit{ubiquitous computing}. The idea was to have computing devices
interconnected and exchanging data about almost everything. This idea was ahead
of his time because there were just a few networks around. And devices such as
cameras and microphones were still quite big, etc. Even though if this definition is
translated to 2017 seems to fit quite perfectly on how devices behave nowadays.
Cars, fridges, watches, smartphones and anything that can have a network
interface is connected to the internet. Huge amounts of data are being published
and stored into the what is now known as \textit{the cloud}. There is not a
clear definition for what \textit{cloud computing} is. For the purposes of this
text, cloud computing could be defined as a pool of shared computing and storage
resources that can be provisioned and released on demand. Given the huge amounts
of data that users store into the \textit{cloud}, companies need to outsource
computing and storing capacity, since building datacenters is a huge investment.
In the last decade, computing infrastructure offering has changed radically.

Companies such as Amazon Web Services are able to offer servers, storage and
services on demand at scale, from 1 server to 1 million servers. Having the
ability to start and stop servers on demand has democratized the ability to
process large amounts of data for many organizations. For example, a startup can
kickoff with just one server to process data. Once the business starts growing,
it can take advantage of the elasticity provided by cloud computing services,
and start using more servers to cope with the load. This is one of the reasons why
data processing systems should be designed to scale out as there are more
computing needs.
Alcaudon, as an elastic data processing system, will be able to adapt to
different loads as the computing needs increase. It will be able to use more
computing resources dynamically as more servers are added to the pool.

\section{Distributed systems}
\label{subsection:distsys}

Distributed systems can be defined as a set of computer programs,
executing on one or more computers, and coordinating actions by
exchanging \textit{messages}~\cite{GuideReliable}. Those computers are
usually located in a \textit{computer network}, a collection of
computers interconnected by hardware that supports message passing and
implement routing protocols. But if this definition is taken to the
extreme, a modern multi-core processor can be characterized as a
distributed system, with many components exchanging information in a
network and coordinating actions to get work done. To some extent, it
is possible to see distributed systems as a super set of concurrent
systems.

A more common example of a distributed system is a user requesting a web page to
a server with his smartphone. This example is a typical \textit{client-server}
model. This \textit{simple} action involves the interaction of various services
such as DNS servers, load balancers, HTTP proxies and servers. All these
services use a network as a mean to interchange messages as shown in figure
~\ref{fig:client-server}.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.7\textwidth]{client-server.pdf}
\caption{Client-server architecture}
\label{fig:client-server}
\end{center}
\end{figure}

As it has been previously discussed, distributed systems are very
common nowadays. They are present in fields like database systems,
internet of things, data processing and many more. There are some
reasons that make more convenient to work with distributed systems,
but the main reason is ability to scale. As defined
in~\cite{cloudadmin}:
%
\begin{quote}
  A system's ability to scale is its ability to process a growing workload,
  usually measured in transactions per second, amount of data or number of
  users.
\end{quote}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.7\textwidth]{partition-rep.pdf}
\caption{Partitioning and Replication}
\label{fig:partitioning}
\end{center}
\end{figure}

Distributed programming can help designing systems with the ability to scale given
the following properties:
%
\begin{itemize}
\item \textit{Reliability}: Achieved by \textit{replication} and/or
  \textit{partitioning} as illustrated in Figure ~\ref{fig:partitioning}.
\item \textit{Elasticity}: Since the systems are designed to work in
  coordination with other processes, it is possible to add more resources to
  handle increasing workloads. But there is an upper limit imposed by the
  coordination model used.
\item \textit{Parallelism}: This is a natural outcome of having multiple resources, they
  can get work done in parallel.
\item \textit{Price/performance ratio}: Scaling vertically (using more powerful
  compute nodes), has upper limits both in available technology and in costs. As
  shown in Figure ~\ref{fig:highend} the performance gap between a high end
  server and a cluster of commodity hardware nodes is tolerable. Another factor
  to take into account when distributed systems are used is the cost of
  networking communications, as it implies an overhead.
\end{itemize}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=1\textwidth]{scalingcost.png}
\caption{Performance advantage of a cluster built with large SMP server nodes
  (128-core SMP) over a cluster with the same number of processor cores built
  with low-end server nodes (four-core SMP), for clusters of varying
  size~\cite{Datacenter}}[\FIXME{pon un caption breve aquí para el LOF}]
\label{fig:highend}
\end{center}
\end{figure}

\subsection{Taxonomy of a distributed system}

Once described what distributed systems are and why they are useful, it is
possible to define a taxonomy of distributed systems topics such as coordination
algorithms, global state collection, distributed consensus, etc.

As it can be seen the topic is wide, so for this documents only
\textit{distributed snapshots}, \textit{membership protocols} and \textit{time
  in distributed systems} will be covered.

\begin{itemize}
\item \textit{Time in distributed systems}:
  Programmers are used to think in sequential programs, where computation steps
  are executed one after the other. The reality in distributed and concurrent
  systems is quite different. Knowledge about global state among all the nodes
  in the system is likely to be outdated. A naive solution to this problem is
  using synchronization based on wall clocks. However, the notion of
  synchronized clocks among nodes is impractical. Protocols like \ac{NTP}~\cite{ntp}
  can have skews up to 1000 seconds, an unacceptable time in some cases. Google
  makes heavy use of hardware-assisted time synchronization using GPS clocks and
  atomic clocks to ensure global consistency for their database
  Spanner~\cite{180268} that can drift up to 100ms. Given the exposed facts, the
  only assumption that can be taken about distributed operations is that they are partially
  ordered. A formal definition for partial order is ~\cite{book:lattices}:
  \begin{definition}{(Partial order)}
    Let $P$ be a set. An order (or partial order) on $P$ is a binary relation
    $\leq$ on $P$ such that, for all $x$, $y$, $z \in P$,
    \begin{enumerate}
    \item $x \leq x$
    \item $x \leq y$ and $y \leq x$ imply $y$
    \item $x \leq y$ and $y \leq z$ imply $x \leq z$
    \end{enumerate}
  \end{definition}
  Given the exposed facts, message order in distributed systems is partial.
  However, there are algorithms that focuses on maintaining a global total order
  of operations~\cite{vclocks}. In environments where the message flow is unbounded
  the notion of completeness is weak. There is no guarantee about when all the
  messages up to a moment $T_n$ have been received or processed. To deal with
  this, Alcaudon uses watermark heuristics. A watermark heuristic defines that up
  to the instant $T_i$ there are high chances that all messages up to that point
  have been received.
\item \textit{Distributed snapshots}:
  Since Alcaudon aims to be a fault-tolerant system, guarantees about persistent
  state must be enforced. To design a resilient system, persistent state
  snapshots should be taken. There are distributed algorithms to deal with this
  problem such as Chandy-Lamport algorithm~\cite{lampsnapshot}. In general, those algorithms are focused
  on taking a global state snapshot that given the constraints presented in the
  previous section is a hard problem to solve. In Alcaudon this obstacle is
  solved by taking snapshots by computation. There is no need of a global
  snapshot since persistent state is local to each computation.

\item \textit{Membership protocols}: Another crucial aspect on an elastic
  distributed system is how to handle cluster membership. It is possible to
  classify them into two categories: static membership and dynamic membership.
  Static membership has a fixed list of cluster members that do not change along
  the life of the cluster. With this static membership there is a chance that at
  some point in time, some members of the cluster will not be available. Since
  cluster membership image is static, thew will be seen as accessible. The other
  approach is to use dynamic membership. It consists on having initial knowledge
  about a subset of the available nodes and later on, via gossiping
  protocols~\cite{gossip} new nodes can \textit{join} the cluster. This mechanism
  is also able to handle when nodes become \textit{unavailable}. For Alcaudon
  the latter membership category is more suitable. Since one of the goals is to
  provide elasticity, in this case the system will be able to be scaled in the
  need of more computing power.
\end{itemize}

\subsection{CRDT}

One of the problems when building distributed systems is sharing state among
different participants. In order to provide a global vision of shared state,
replication~\cite{book:replication} algorithms have been developed during the last
years. Nevertheless, these algorithms have certain limitations in terms of
performance and scalability due to serialization. Eric Brewer postulated the
conjecture that it is impossible for a distributed system to provide
consistency, availability and partition-tolerance guarantees. Where
they can be defined as:

\begin{itemize}
\item \textit{Consistency}: A global total order of operations is provided,
  meaning that all nodes see the same data.
\item \textit{Availability}: The system is able to respond to requests even in
  the presence of partial failures.
\item \textit{Partition tolerance}: The system is able to continue operating in
  the event of network failures.
\end{itemize}

This conjecture was proved~\cite{capproof} later and it is known nowadays as the CAP
theorem(\ref{fig:cap}). This theorem establishes a trade-off between consistency (total order
of operations) and partition-tolerance.

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.5\textwidth]{cap.pdf}
  \caption{CAP theorem}
  \label{fig:cap}
\end{figure}

There are systems where total order of operations is not mandatory, these systems
choose to be partition tolerant. They are known as \textit{eventual consistent}
systems. A replica executes an operation without synchronizing its new state with
other replicas. This operation is sent asynchronously to the rest of replicas.
Every replica eventually applies the updates, without any guarantee about the
order in which these updates are applied. This approach establishes that data is
available even during network partitions. One of the problems of eventual
consistent systems is conflicting updates. Systems such Dynamo~\cite{dynamo}
allows to choose if clients should resolve conflicts or use simple approaches as
`last write wins'.

\acf{CRDT} provides a model based on simple mathematical properties that ensures
eventual consistency. \acs{CRDT}s do not require synchronization, since updates
are executed immediately. This property allows \acs{CRDT}s to be scalable and
fault-tolerant. In order to guarantee convergence in \acs{CRDT}s, data types
should follow the next properties:

A join semilattice~\cite{book:lattices} is a partial order $\leq_{v}$ equipped with a
least upper bound (LUB) $\sqcup_v$, defined as follows:

\begin{definition}{(Least Upper Bound (LUB))}
  $m = x \sqcup_v y$ is a Least Upper Bound of $\{x, y\}$ under $\leq_{v}$ \textit{iff}
  $x\leq_{v}m$ and $y\leq_{v}m$ and there is no $m'\leq_{v}m$ such that $x\leq_{v}m'$ and $y\leq_{v}m'$.
\end{definition}

It follows from the definition that $\sqcup_v$ is commutative: $x \sqcup_v y =_v y \sqcup_v x$; idempotent:
$x \sqcup_v x =_v x$; and associative: $(x \sqcup_v y) \sqcup_v z =_v x \sqcup_v (y \sqcup_v z)$.


\begin{definition}{(Join Semilattice)}
  An ordered set ${S, \leq_v}$ is a \textit{Join Semilattice} \textit{iff}
  $\forall x, y \in S, x \sqcup_v y$ exists.
\end{definition}

A \acs{CRDT} whose payload takes its values in a semilattice and where
\textbf{merge}$(x,y) = x \sqcup_v y$, converges towards the LUB of the initial
and updated values.

Given the described properties, \acs{CRDT}s are a good fit for highly scalable systems
such as Alcaudon aims to be. For contexts such as cluster membership or non-centralized
replication \acs{CRDT}s could be used in order to implement different Alcaudon sub-systems.

\subsection{Distributed system reliability}

Building distributed systems is not an easy task. In this subsection, different
failure scenarios that can happen in a distributed environment will be presented.

As proposed by ~\cite{GuideReliable}, it is possible to classify system error
causes as follows:
\begin{itemize}
\item \textit{Hardware failures}: These kind of failures are inevitable, since
  hardware has a life cycle and some components might fail or maintenance for
  components should be taken care of.
\item \textit{Software failures}: Software is becoming more and more complex.
  Software complexity errors such as coding errors, misguided software design,
  memory leaks or inadequate specifications are inevitable. There are countless
  examples of millionare losses, like when in 1999 NASA lost a Mars orbiter
  spacecraft because one team was working with Imperial units while another used
  metric units.
\item \textit{Complexity}: Developing software in distributed systems is
  complex. The notion of global state or order is fuzzy. Asynchronous
  communication is the norm. Regular developers are used to sequential
  computations, and getting familiarized to work within this environment is hard
  and can lead to bogus implementations. Luckily the use of formal verification
  is growing in the field, using tools such as TLA+~\cite{tla} by Lamport. The
  usage of these techniques helps to tackle the inherent complexity with strong
  foundations.
\item \textit{Lack of failure detectors}: Most distributed systems try to detect
  failure just by using timeouts. This measure is quite straightforward to
  implement and reason about. The main drawback is that these techniques are
  raw. Vogels ~\cite{vogels} proposes a refinement over plain timeouts for
  failure detection, using more meta-information about the environment where
  the distributed system is running; reasoning about operating system metrics,
  networking metrics and disseminated probes, failure detection can improve.
\item \textit{Hostile environments}: There has been enumerated some reliability
  threats, which distributed systems need to deal with. Those threats are in
  \textit{control}, meaning that the developers of the system should anticipate
  them. Designing the systems so they are ready to deal with the exposed
  problems. However those systems run in shared networks where security threats
  are a reality. It is a wide field and it is becoming more and more important
  to take all the possible measures to avoid security vulnerabilities.
\end{itemize}

As a final note about other problems, there are costs associated with
distributed programming; network latency, fault-tolerance mechanisms and
consensus can have impact in the throughput achieved by a system. In
~\cite{189908} a metric named Configuration that Outperforms a Single Thread
(COST), is proposed. It measures the number of cores needed to outperform a
single threaded implementation. In these experiments, there are many distributed
processing frameworks that don't perform properly when compared to single node
implementations. As a conclusion, there are scenarios where the costs associated
with distributed computing are higher than the gains.

Alcaudon's goal is to provide users a simple but powerful interface that enables
them to parallelize and distribute computations for unbounded datasets without all
the described problems.

\subsection{Actor Model}

To tackle all the complexity described in previous sections there is a
computation model that is well suited: the actor model.

The actor model provides a high level abstraction to deal with concurrent and
distributed systems. The theoretical model was developed by Carl Hewitt in
the seventies, but it was popularized by Erlang programming language~\cite{erlang}
developed by Ericsson.

It can be characterized as follows:
\begin{itemize}
\item Actors communicate via asynchronous messages
\item Actors manage their state
\item In the event of a message response they can:
  \begin{itemize}
  \item Create new child actors
  \item Send messages to another actors
  \item Stop actors (even themselves)
  \item Change their behavior for the next messages
  \end{itemize}
\end{itemize}

Since distributed systems communicate asynchronously, this model fits perfectly,
because, even in single node environments, all the computations are executed in
response to asynchronous messages. The fact that the shared state among actors
is minimal helps to tackle all the inherent complexities around concurrency.
Another interesting outcome from designing the systems using message passing is
that creating protocols comes naturally in this model. The ability to change
their behavior depending on the incoming messages makes easy to model finite
state machines that fit perfectly with protocol design.

There are many implementations available\FIXME{tag extraño en la referencia}~\cite{wikiactor}, but the more popular
and stable versions are the Erlang programming language and Akka. Both versions
have similar features, in fact, Akka is highly inspired by Erlang. Nevertheless
Akka is built on top of the \ac{JVM}, making it the most interesting implementation
available due to the vast ecosystem accessible around the \acs{JVM}.

Akka has some extensions to the actor model that make it quite interesting to
build distributed systems.
%
\begin{itemize}
\item Actor supervision: One interesting approach of actor based design is how
  to handle failures. Each actor has a supervisor that handles its life-cycle,
  exceptions included. With this supervision schema it is easier to isolate failure
  scenarios into specific actors instead of affecting the whole system. Each
  supervisor can define a strategy to apply when a child actor fails, i.e.
  restarting the actor or stopping it. This is highly beneficial because error
  handling logic is separated from business logic. These patterns resemble how
  ships are built, where the hull is compartmentalized into different watertight
  bulkheads. Therefore, if the hull is broken, the failure is limited to that
  bulkhead without compromising the ship stability. Alcaudon will benefit from
  this pattern to achieve fault-tolerant computations. One actor supervision
  tree can be found in figure~\ref{fig:supervision}.
\item Logical addresses: Each actor has a virtual address based on the actor
  system and its hierarchy. This helps providing location transparency, as
  actors can change their physical location and the framework will take care of
  routing messages to the new actor location.
\item Clustering: Support for distributed actors is provided, combined with
  logical addresses facilitates writing distributed applications.
\item Persistence: Akka supports actor state persistence. It is implemented as
  an append-only log. That technique is commonly used as transaction log in
  traditional databases, each change is appended to the transaction log. In case
  of a failure, those events are applied again to the in memory database
  buffers. To avoid long recovery times, snapshots from memory are taken
  regularly so the recovery process just needs to re-apply changes from that
  point. Akka persistence is implemented as described. This feature fits very
  well to mantain Alcaudon persistent state of computations, since computations
  can be defined as actors.
\end{itemize}

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.5\textwidth]{supervision.pdf}
\caption{Actor supervision tree}
\label{fig:supervision}
\end{center}
\end{figure}

In conclusion, the actor model is a good computational model to base Alcaudon
on. It has properties that allows tackling all the presented difficulties around
writing distributed systems properly.

\section{Job scheduling}

Computations in distributed data processing clusters are expressed as a
dataflows. They can be considered as a direct acyclic graph (DAG) as the one in
Figure~\ref{fig:dataflow}.

\begin{figure}[h!]
\begin{center}
\includegraphics[width=0.6\textwidth]{dataflow.png}
\caption{Dataflow}
\label{fig:dataflow}
\end{center}
\end{figure}

One open problem in the construction of distributed data processing systems is
how to place those computations represented as a \acs{DAG}, given a set of computing
nodes. This problem is known as the flexible Job-shop scheduling problem and it
has been widely studied during the last 60 years. One graphical example for $3$
jobs to be scheduled along $3$ machines can be seen in Figure~\ref{fig:jssp}.
As defined in~\cite{jobshop2}:

\begin{definition}
In the flow shop scheduling problem we have a set of $n$ jobs that must be
processed on a given set of $m$ machines that are located in a fixed order. Each
job $j$ consists of a sequence of $m$ operations, where the $i$-th operation
must be processed during $p_{ij} \in \Z^{+}$ Time units without interruption on
the $i$-th machine. A feasible schedule is one in which each operation is
scheduled only after all operations preceding it in its job have been completed,
and each machine processes at most one operation at a time. A natural
generalization of the flow shop problem is to not require jobs to be processed
on all machines, i.e., a job still requests the machines in compliance with
their fixed order but may skip some of them. We will refer to this more general
version as generalized flow shops or flow shops with jumps. Generalized flow
shop (and thus flow shop) scheduling is a special case of the acyclic job shop
scheduling, which only requires that within each job all operations are
performed on different machines, which in turn is a special case of the general
job shop scheduling, where each job may have many operations on a single
machine. For any given schedule, let $C_j$ be the completion time of the last
operation of job $j$. We consider the natural and typically considered
objectives of minimizing the makespan, $C_{max} = max_j C_j$ , and the
\textit{sum of weighted completion times}, $\sum w_jC_j$ , where $w_j$ are
positive integers. The goal is to find a feasible schedule which minimizes the
considered objective function.
\end{definition}

\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{jssp.png}
\caption{ A Gantt-Chart representation of a solution for a $3m \times 3j$ job scheduling problem~\cite{geneticbook}}
\label{fig:jssp}
\end{center}
\end{figure}

Computing an optimal solution for this problem is a well known NP-hard
problem~\cite{nphard}. Given this fact, all the proposed algorithms to solve this
problem are approximation algorithms. Some of the techniques used are listed
below:

\begin{itemize}
\item Algorithms based on priority rules and active schedule generation
\item \textit{Shifting bottleneck}
\item Simulated annealing
\item Tabu search
\item Genetic algorithms
\end{itemize}

Alcaudon will use an state-of-the-art scheduling framework,
Firmament~\cite{firmament}, in order to solve this problem.

\section{Library design}

Humans have been using abstraction since the very beginnings of their existence.
The reality around is too complex to understand every detail of it. Thus,
abstraction is a really excellent tool to extract just the relevant facts about
the studied phenomenons. Computer Science, Software Engineering and Electrical
Engineering combine different abstractions in order to to build complex systems
in a manageable way.
Computer Science provides the tools whose roots are based on mathematical
foundations such as automaton theory, lambda calculus, language design, data
structures, etc.
Using those ideas, software engineering can be seen as the bridge between
hardware and computer science. It builds abstractions based on both worlds such
as operating systems, that hides the details of specific hardware and exposes an
uniform interface. And the list goes on.

The main goal of abstractions is to
provide ways to reason about different problems with a limited scope.
As it has been exposed, providing tools that hide all the complexities is key to
build more complex systems. One of the main goals of the field since the late
60s has been to build reusable software~\cite{reuse}. There have been many
advances in code reuse, from libraries and frameworks that are used to build
software on top them, to \acf{SaaS}. SaaS exposes public \ac{HTTP}
\ac{API} as the entry point to their functionality. This category of software reuse
can be the end of the spectrum, since the client just has knowledge about
the exposed interface and does not know anything about the underlying parts of the
system.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{serverless.png}
\caption{Serverless architecture example~\cite{awsserverless}}
\label{fig:serverless}
\end{center}
\end{figure}

One recent trend in industry is serverless architecture like the one shown in
Figure~\ref{fig:serverless}. This programming model has foundations in reactive
programming, and it is based on providing code to produce computation results as
response to changes or events. In this paradigm it is just needed to provide
code implementing certain interface~\ref{code:lambdalisting} to handle events,
such as database changes, and the system will take care of deploy and run those
pieces together. As it can be observed, abstraction is going upwards and
programmers just need to take care of writing business logic and building
robust programs.

\begin{lstlisting}[language=java, frame=trBL, label=code:lambdalisting, float=ht, caption = {AWS Lambda Interface}]
  public interface RequestStreamHandler {
    public void handleRequest(InputStream inputStream, OutputStream outputStream, Context context)
    throws IOException
  }
\end{lstlisting}

Alcaudon philosophy is similar to the serverless architecture. The goal is to
provide abstractions around unbounded datasets so programmers just take care of
defining their computations and dependencies. Alcaudon will provide a simple
interface so the details to know about the system are minimized.

% Local Variables:
% ispell-local-dictionary: "american"
% End:
