\selectlanguage{spanish}
\chapter{Resumen}

En los últimos años, la cantidad de datos recolectados no ha parado de crecer.
Las empresas de internet registran todas las acciones de sus usuarios para
proveer mejores experiencias. Los experimentos científicos generan grandes
cantidades de datos. El auge del \textit{Internet de las Cosas} ha traido
consigo un aumento de los datos a almacenar, por ejemplo los coches autónomos
necesitan analizar grandes cantidades de información para funcionar
correctamente. Estos hechos, combinados con el decreciente coste del
almacenamiento de datos, ha hecho que distintas organizaciones aprecien el
potencial de sus datos y busquen distintas formas de analizarlos.

Las empresas deben ganar ventaja competitiva usando sus datos a la hora de tomar
decisiones en lugar de basar las mismas en intuiciones. Debido a esta tendencia,
diversos sistemas para el análisis de grandes cantidades de datos han surgido en
los últimos años. Esos sistemas fueron diseñados para analizar grandes
cantidades de datos históricos. Sin embargo, en los últimos tiempos se está
dando una creciente necesidad de extraer conocimiento de los datos tan pronto
como sea posible. Esto ayuda a las organizaciones a reaccionar de una forma ágil
ante eventos de divérsa índole.

Este proyecto, Alcaudon, tiene como objetivo implementar un sistema de análisis
distribuido para datos en \textit{streaming}. Alcaudon permitirá analizar
fuentes de datos infinitas en \textit{tiempo real}. Uno de los objetivos de este
proyecto es proveer un modelo de programación para la creación de computaciones
distribuidas, facilitando la creación de \textit{pipelines} para el análisis de
datos en \textit{streaming}. Además, Alcaudon ha sido diseñado teniendo en
cuenta criterios de escalabilidad y tolerancia a fallos. Por tanto, este
proyecto presenta una herramienta robusta y potente para el procesado y análisis
de fuentes de datos infinitas.

\selectlanguage{english}
\chapter{Abstract}

Currently, the amount of collected data is growing. Internet companies track all
user actions in order to provide better experiences. Science experiments
generate colossal amounts of data. Another growing source of data is Internet
of Things, i.e. autonomous cars need to collect and analyze large amounts of
data in order to function properly. These, combined with the plunging cost of
data storage makes storing large amounts of information for later analysis very
compelling.

Organizations should take advantage of all these data in order to overcome
ad-hoc business decisions in favor of data-driven approaches. Therefore, to
satisfy data analysis needs, many data processing systems have appeared in
the last years. Those systems were focused on processing large amounts of
historical data. However, there is an increasing need to acquire knowledge
from data as soon as possible, allowing a prompt reaction to a wide range of
events.

This project aims to implement a distributed streaming data processing platform,
Alcaudon, allowing near real-time data analysis of unbounded data-sets. One of
the objectives of this project is to provide a powerful programming model,
making it straightforward to write distributed data analysis pipelines.
Moreover, Alcaudon has been designed contemplating scalability and
fault-tolerance. Therefore, this project presents a robust and potent tool to
process and analyze unbounded data-sets.
