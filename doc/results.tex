\chapter{Results}

In this chapter, attained results during the development of this project will be
presented. First, a series of benchmarks have been used in order to exercise key
Alcaudon modules and measure how the system behaves. Then, a possible real
application for Alcaudon will be stated. To conclude, a summary of accomplished
objectives is presented.

\section{Benchmarks}

Alcaudon has been tested thoroughly, both using traditional testing techniques
as well as more state-of-the-art techniques such as property based testing.
These tests can guarantee correctness in terms of behavior, but they do not
measure system precision in terms of performance goals. Even though Alcaudon
has been designed carefully, using the adequate data structures and taking
care of performance, empiric results are needed. It is known that creating
accurate benchmarks is complex~\cite{benchbias}, as there are many factors
that can lead to misleading results. Presented benchmarks are implemented using
JMH\footnote{http://openjdk.java.net/projects/code-tools/jmh/}.

\subsection{Computation execution benchmark}

This benchmark measures the number of computations per second that Alcaundon can
handle.

\FIXME{TODO -> figure}

\subsection{Stream benchmark}

This benchmark measures the number of records that a stream can handle.

\FIXME{TODO -> figure}

\subsection{Record router benchmark}

This benchmark measures the number of records that a router can route.

\FIXME{TODO -> figure}

\section{Alcaudon application}

Once Alcaudon has been described in detail, its full potential can be unveiled
in a real world scenario. For this example, a public
public\footnote{http://www.nyc.gov/html/tlc} data set of the New York City Taxi
and Limousine Commission (TLC) will be used. This data set contains records
about taxi trips in New York from different years. The goal of this example is
to use this data set as a real unbounded data source, where taxi customers take
taxis for a ride from point A to point B and this information is delivered into
Alcaudon. With this information it is possible to build an Alcaudon dataflow
topology in order to detect spikes in rides to certain parts of the city, for
example during a concert. For companies like Uber, where prices fluctuate
depending on demand, processing this information in \textit{real time} is
fundamental. This example will build an Alcaudon dataflow topology in order to
aggregate rides per zone and emit results in constant time windows. These
aggregated results will be published into ElasticSearch to be visualized later
on as shown in figure~\ref{fig:rides}. Business stakeholders can use these
dashboards in order to take decisions with the latest information available.

\begin{figure}[!h]
\begin{center}
\includegraphics[width=0.8\textwidth]{nyrides.jpg}
\caption{Example dashboard}
\label{fig:rides}
\end{center}
\end{figure}

Since this is a synthetic example, a custom source has been defined in order to
emit ride events using their original timestamps. Ride information contains
different fields, but for the purposes of this example, a simple representation
has been created, listed in~\ref{code:ride}. In order to aggregate location
information, the map is divided into cells, where each ride belongs to one of
these cells. This subdivision in cells is used in order to define a key per
record as it can be found in listing~\ref{code:keyExtractorNY}.

The main goal of this example is to publish updated data about the number of
passengers taking rides in different places of the city, to later on visualize
this data in a dashboard. First of all, rides that are not taking place in New
York are filtered out~(\ref{code:computationFilterRide}) and published into a new
stream. These filtered rides are consumed by
RidePassengerCountComputation~\ref{code:computationRide} that it is responsible
for keeping the number of passengers per cell and update them when new rides
arrive into the system. A timer is set-up, when this timer is triggered, the
information stored up to that point, in this case the number of ride passengers
per cell, is published into a Sink. In addition to counting the number of
passengers, this computation publishes into a different sink when the number of
passengers arriving into a specific cell is bigger than a setting. This is
useful when alerts for different events are needed. Dataflow topology definition
can be found in listing~\ref{code:rideDataflow}. This example shows the
potential of unbounded data-set processing, and the strengths of Alcaudon in
particular.

\begin{lstlisting}[language=scala, frame=trBL, label=code:ride, float=ht, caption = {Ride \acs{ADT}}]
case class Ride(id: Long, time: DateTime, location: Point, passengerCount: Int)
\end{lstlisting}

\begin{lstlisting}[language=scala, frame=trBL, label=code:computationFilterRide, float=ht, caption = {Computation to filter out non New York rides}]
class FilterRideComputation extends Computation {
  def processRecord(record: Record): Unit = {
    val ride: Ride = deserialize(record.value)
    if (ride.isInNYC())
      produceRecord(nycRides, record)
  }

  def processTimer(timeStamp: Timer): Unit = {}
}
\end{lstlisting}

\begin{lstlisting}[language=scala, frame=trBL, label=code:keyExtractorNY, float=ht, caption = {Key extractor function}]
class RideKeyExtractor extends KeyExtractor {
  def extractKey(msg: Array[Byte]): String = {
    val ride: Ride = deserialize(msg)
    Utils.locationToCell(ride.location)
  }
}
\end{lstlisting}

\begin{lstlisting}[language=scala, frame=trBL, label=code:computationRide, float=ht, caption = {Ride \acs{ADT}}]
class RidePassengerCountComputation extends Computation {
  def processRecord(record: Record): Unit = {
    val ride: Ride = deserialize(record.value)
    val count: Int = get(record.key) + ride.passengerCount
    if (count > settings.alertPassengers)
      produceRecord(alertSink, RawRecord(serialize(record.key), record.timestamp))
    set(count + ride.passengerCount)
    setTimer(FixedRecurrentTimer(cell, 5.minutes))
  }

  def processTimer(timeStamp: Timer): Unit = {
    val location = Utils.cellToLocation(timer.tag)
    val totalPassengerCount = get(timer.tag)
    produceRecord(elasticSink, RawRecord(serialize((location, totalPassengerCount)), record.timestamp))
  }
}
\end{lstlisting}

\begin{lstlisting}[language=scala, frame=trBL, label=code:rideDataflow, float=ht, caption = {Ride dataflow creation}]
val dataflow = DataflowBuilder("ridesCounter")
  .withSource("taxiRides", TaxiRides())
  .withComputation("filterNYRides",
    FilterRideComputation,
    OutputStreams("nycRides"),
    AlcaudonInputStream("taxiRides")
  .withComputation("passengerCount",
    RidePassengerCountComputation,
    OutputStreams("elasticSink", "alertSink"),
    AlcaudonInputStream("nycRides", new RideKeyExtractor())
  .withSink("elasticSink", ElasticSearchSink)
  .withSink("alertSink", HTTPSink(settings.alertEndpoint))
  .build()
\end{lstlisting}

\section{Accomplished objectives}

During the development of this project, all objectives defined at the beginning
of this document have been accomplished. Alcaudon provides all the necessary
tools to deploy distributed stream data processing pipelines. It attains this
objective by supplying an abstract model, the Computation \acs{API}, that users
implement, leveraging the all the complexities around fault-tolerant distributed
models to Alcaudon.

Regarding specific objectives, these have been accomplished as well:
%
\begin{itemize}
\item Alcaudon provides an abstraction in order to create distributed
  computations; computation \acs{API} alongside the dataflow builder.
\item The system provides exactly-once processing semantics, using at-least once
  delivery in combination with idempotent record processing. In order to achieve
  idempotence, state-of-the-art probabilistic data structures have been used as
  well as persistent actors to guarantee persistent state durability.
\item The system provides watermark based timers in order to work with
  out-of-order data. The implementation of this subsystem uses state-of-the-art
  coordination-free distributed data types in order to replicate knowledge about
  time event evolution inside the system.
\item Dataflow \acs{DAG}s are scheduled into available compute nodes using an
  state-of-the-art flexible cluster scheduler~\cite{firmament}. Different scheduling
  policies can be configured depending on the needs, where the default choice is
  biased towards co-location.
\item Several sources and sinks are provided by default such as Twitter streaming
  API, TCP Sockets or Apache Kafka. However it is possible to implement new ones
  just extending SourceFn and SinkFN interfaces.
\item Alcaudon provides an elastic distributed implementation where compute
  nodes can join the cluster in the events of burst of load. The system has been
  designed in order to be resilient and fault-tolerant. The actor model has
  facilitated the development in terms of resilience due to is supervision mechanism that
  allows to isolate failure.
\item Alcaudon provides different metrics about the state of the system. It has been
  integrated with industry proven technologies in the space of monitoring such as
  Prometheus and Graphana. Using these tools it is possible to create a rich set
  of alerts and dashboards giving a good overview of how the different components
  of Alcaudon perform.
\item Alongside the previously presented results, Alcaudon is distributed using Docker
  containers and its library is available in SonaType repository. This makes almost
  trivial to deploy a full-featured Alcaudon cluster.
\end{itemize}

\section{Future work}

Given that Alcaudon has been designed in a modular way and it has many automatic
tests, including new features should be effortless. In this section some future work
that could be done to the platform is presented.

\begin{itemize}
\item Improve system performance avoiding object allocations in certain
  sensitive parts of the system such as the Streams. High allocation rate
  usually hurts managed systems performance due to garbage collection. Some
  systems such as Netty\footnote{https://netty.io/} use pre-allocated memory buffers
  in order to avoid heap allocations as much as possible. This approach could be
  explored in order to improve Alcaudon performance.
\item Implement a machine learning algorithm in order to select a scheduling policy
  based on similar already run dataflow pipelines, improving the overall performance.
\item Implement a fully functional programming interface on top of the
  computation API in order to offer a more expressive API to the users.
  Providing \textit{combinators} such as map, groupBy, filter, etc.
\item Alcaudon watermark algorithm is just a minimal version. Used heuristic
  could be improved and it would be interesting to test different machine
  learning algorithms in order to predict watermarks. Another interesting
  approach to the problem that watermarks solve is to use virtual tables as
  Apache Kafka Streaming does~\cite{kafkastreams}.
\end{itemize}
